{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert-BiLSTM\n",
    "\n",
    "This notebook performs training and testing of the Bert-BiLSTM model for TSA on the IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import gc, os, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from transformers import AutoModel\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 0): # Define seed for reproducability\n",
    "    '''\n",
    "    set random seed\n",
    "    '''\n",
    "    # random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert-BiLSTM Model definition\n",
    "class BERT_Bi_Arch(nn.Module): \n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Bi_Arch, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.lstm = nn.LSTM(768, 256, batch_first=True,bidirectional=True)\n",
    "        self.linear = nn.Linear(256*2, 2)       \n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "        sequence_output, pooled_output = self.bert(sent_id, attention_mask=mask, return_dict=False) \n",
    "        lstm_output, (h,c) = self.lstm(sequence_output) # extract the 1st token's embeddings\n",
    "        hidden = torch.cat((lstm_output[:,-1, :256],lstm_output[:,0, 256:]),dim=-1)\n",
    "        linear_output = self.linear(hidden.view(-1,256*2))\n",
    "        return self.softmax(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits dataset into train and test, converts class label values to integer representation\n",
    "def read_dataset(df): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.3, shuffle=True) # train test split of 0.3\n",
    "    \n",
    "    # map labels to integers\n",
    "    polarity_class = {\"negative\":0, \"positive\":1} # binary classes\n",
    "    y_train = y_train.apply(lambda x: polarity_class[x])\n",
    "    y_test = y_test.apply(lambda x: polarity_class[x])\n",
    "    \n",
    "    return X_train.tolist(), y_train, X_test.tolist(), y_test\n",
    "\n",
    "# convert series to list\n",
    "def pre_process_dataset(values): \n",
    "    new_values = list()\n",
    "    \n",
    "    for value in values:\n",
    "        new_values.append(value)\n",
    "    return new_values\n",
    "\n",
    "# tokenize sentence inputs and generate attention masks\n",
    "def data_process(data, labels): \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    for sentence in data:\n",
    "        bert_inp = bert_tokenizer.__call__(sentence, max_length=150, \n",
    "                                           padding='max_length', pad_to_max_length=True,\n",
    "                                           truncation=True, return_token_type_ids=False)\n",
    "        input_ids.append(bert_inp['input_ids'])\n",
    "        attention_masks.append(bert_inp['attention_mask'])\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    attention_masks = np.array(attention_masks)\n",
    "    labels = np.array(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# preprocesses and return data in terms of ids, masks and labels\n",
    "def load_and_process(df): \n",
    "    train_data, train_labels, test_data, test_labels = read_dataset(df)\n",
    "\n",
    "    train_input_ids, train_attention_masks, train_labels = data_process(pre_process_dataset(train_data), train_labels)\n",
    "    test_input_ids, test_attention_masks, test_labels = data_process(pre_process_dataset(test_data), test_labels)\n",
    "\n",
    "    return train_input_ids, train_attention_masks, train_labels,\\\n",
    "           test_input_ids, test_attention_masks, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Function\n",
    "def train(model, loss_function, batch_size, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = [] # empty list to save model predictions\n",
    "\n",
    "    # iterate over batches\n",
    "    total = len(train_dataloader)\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        step = i+1\n",
    "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = 'â–ˆ' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='') # accuracy={total_accuracy}\n",
    "\n",
    "        batch = [r.to(device) for r in batch] # push the batch to gpu\n",
    "        sent_id, mask, labels = batch\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model.zero_grad() # clear previously calculated gradients\n",
    "        preds = model(sent_id, mask) # get model predictions for the current batch\n",
    "        labels = labels.type(torch.LongTensor) # make sure it labels are int64 type\n",
    "        \n",
    "        loss = loss_function(preds, labels.to('cuda')) # compute the loss between actual and predicted values\n",
    "        total_loss += float(loss.item()) # add on to the total loss\n",
    "        loss.backward() # backward pass to calculate the gradients\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        optimizer.step() # update parameters\n",
    "\n",
    "        total_preds.append(preds.detach().cpu().numpy()) # append the model predictions\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_loss = total_loss / (len(train_dataloader)*batch_size) # compute the training loss of the epoch\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "    \n",
    "    return avg_loss, total_preds # returns the loss and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "def load_data(df):\n",
    "    # load dataset\n",
    "    train_input_ids, train_attention_masks, train_labels,\\\n",
    "    test_input_ids, test_attention_masks, test_labels = load_and_process(df)\n",
    "\n",
    "    train_df = pd.DataFrame(list(zip(train_input_ids, train_attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "    test_df = pd.DataFrame(list(zip(test_input_ids, test_attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "    \n",
    "    # import BERT Model and BERT Tokenizer\n",
    "    bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Tokenization \n",
    "    # for train set\n",
    "    train_seq = torch.tensor(train_df['input_ids'].tolist())\n",
    "    train_mask = torch.tensor(train_df['attention_masks'].tolist())\n",
    "    train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "    # for test set\n",
    "    test_seq = torch.tensor(test_df['input_ids'].tolist())\n",
    "    test_mask = torch.tensor(test_df['attention_masks'].tolist())\n",
    "    test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "    # Create DataLoaders \n",
    "    batch_size = 16 \n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y) # wrap tensors\n",
    "    train_sampler = RandomSampler(train_data) # sampler for sampling the data during training\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # dataLoader for train set\n",
    "\n",
    "    return bert, batch_size, train_dataloader, train_y, test_y, train_seq, test_seq, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv') # read df\n",
    "bert, batch_size, train_dataloader, train_y, test_y, train_seq, test_seq, train_mask, test_mask= load_data(df) # preprocess dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wenro\\AppData\\Local\\Temp\\ipykernel_6924\\1508065955.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  train_seq = torch.tensor(train_df['input_ids'].tolist())\n",
      "c:\\Users\\wenro\\anaconda3\\envs\\CZ4042_NN_Proj\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "NO1\n",
      "Batch 2188/2188 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.00% complete, loss=0.02\n",
      "\n",
      "Training Loss: 0.020\n",
      "\n",
      "Epoch 2 / 3:\n",
      "NO1\n",
      "Batch 2188/2188 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.00% complete, loss=0.01\n",
      "\n",
      "Training Loss: 0.013\n",
      "\n",
      "Epoch 3 / 3:\n",
      "NO1\n",
      "Batch 2188/2188 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.00% complete, loss=0.01\n",
      "\n",
      "Training Loss: 0.008\n",
      "Loading polarity weights\n",
      "Loaded polarity weights!\n",
      "\n",
      "Predicting Results...\n"
     ]
    }
   ],
   "source": [
    "# Train loop\n",
    "for param in bert.parameters(): # freeze all the parameters, we are performing pre-training\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = BERT_Bi_Arch(bert).to(device) # pass the pre-trained BERT to our defined architecture\n",
    "optimizer = AdamW( # define the optimizer\n",
    "    model.parameters(),\n",
    "    lr = 5e-5, \n",
    "    eps = 1e-8\n",
    ")\n",
    "\n",
    "loss_function = nn.NLLLoss() # loss function\n",
    "\n",
    "best_loss = float('inf') # set initial loss to infinite\n",
    "\n",
    "epochs = 3\n",
    "current = 1\n",
    "train_loss_list = []\n",
    "\n",
    "while current <= epochs: # for each epoch\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    train_loss, _ = train(model, loss_function, batch_size, train_dataloader, optimizer) # train model\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    # evaluate model\n",
    "    # valid_loss, _ = evaluate(model, loss_function, batch_size, val_dataloader, task)\n",
    "\n",
    "    # save the best model\n",
    "    if os.path.isfile('polarityBertBiLSTM.pth') == False:\n",
    "        torch.save(model.state_dict(), 'polarityBertBiLSTM.pth')\n",
    "        best_loss = train_loss_list[current-1]\n",
    "        \n",
    "    if len(train_loss_list) > 1:\n",
    "        if train_loss_list[current-1] < best_loss:\n",
    "            best_loss = train_loss_list[current-1]\n",
    "            torch.save(model.state_dict(), 'polarityBertBiLSTM.pth')\n",
    "\n",
    "                \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n",
    "\n",
    "else:\n",
    "    # load weights of best model\n",
    "    print(\"Loading polarity weights\")\n",
    "    model.load_state_dict(torch.load(\"polarityBertBiLSTM.pth\"))\n",
    "    print(\"Loaded polarity weights!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters (BERT-BiLSTM): 111584514\n"
     ]
    }
   ],
   "source": [
    "# Get total trainable parameters for BERT-BiLSTM\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "for param in bert.parameters(): # freeze all the parameters, we are performing pre-training\n",
    "    param.requires_grad = False\n",
    "model = BERT_Bi_Arch(bert).to(device) \n",
    "model.load_state_dict(torch.load(\"./stored_weights/polarityBertBiLSTM.pth\"))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Trainable Parameters (BERT-BiLSTM): {pytorch_total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict:  220.23912048339844\n",
      "Accuracy: 0.9583333333333334\n",
      "Precision: 0.9584075854988081\n",
      "Recall: 0.9583324881481332\n",
      "F1-score: 0.9583315908530592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96      7499\n",
      "           1       0.95      0.96      0.96      7501\n",
      "\n",
      "    accuracy                           0.96     15000\n",
      "   macro avg       0.96      0.96      0.96     15000\n",
      "weighted avg       0.96      0.96      0.96     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "model = BERT_Bi_Arch(bert).to(device) \n",
    "model.load_state_dict(torch.load(\"../stored_weights/polarityBertBiLSTM.pth\")) # load trained model\n",
    "model.eval()\n",
    "\n",
    "batch_size = 16\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y) # wrap tensors\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size) # dataLoader for train set\n",
    "\n",
    "total_preds = []\n",
    "\n",
    "start = time.time()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = [r.to(device) for r in batch] # push the batch to gpu\n",
    "    sent_id, mask, labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        preds = model(sent_id.to(device), mask.to(device))\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    total_preds.append(preds) # append the model predictions\n",
    "    \n",
    "end = time.time()\n",
    "print(\"Time taken to predict: \", end - start) # time taken for prediction\n",
    "  \n",
    "total_preds = np.concatenate(total_preds, axis=0)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(test_y, total_preds, average='macro') # calculate metrics\n",
    "accuracy = accuracy_score(test_y, total_preds) # accuracy score\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {fscore}\")\n",
    "print(classification_report(test_y, total_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
