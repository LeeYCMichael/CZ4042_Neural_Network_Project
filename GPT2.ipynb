{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2\n",
    "This notebook performs training and testing of the GPT2 model for TSA on the IMDB movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import gc\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler \n",
    "from transformers import (GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          GPT2ForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 0): # Define seed for reproducability\n",
    "    '''\n",
    "    set random seed\n",
    "    '''\n",
    "    # random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data into train test sets of tokens\n",
    "def read_dataset(df):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.3, shuffle=True) # train test split of 0.3\n",
    "    \n",
    "    # map labels to integers\n",
    "    polarity_class = {\"negative\":0, \"positive\":1} # binary classes\n",
    "    y_train = y_train.apply(lambda x: polarity_class[x])\n",
    "    y_test = y_test.apply(lambda x: polarity_class[x])\n",
    "    \n",
    "    return X_train.tolist(), y_train, X_test.tolist(), y_test\n",
    "\n",
    "# series to list\n",
    "def pre_process_dataset(values):\n",
    "    new_values = list()\n",
    "    \n",
    "    for value in values:\n",
    "        new_values.append(value)\n",
    "    return new_values\n",
    "\n",
    "# tokenize data to input_ids, attention_masks\n",
    "def data_process(data, labels):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    for sentence in data:\n",
    "        inp = tokenizer(sentence, max_length = 150, padding='max_length', truncation=True, return_token_type_ids=False)\n",
    "\n",
    "        input_ids.append(inp['input_ids'])\n",
    "        attention_masks.append(inp['attention_mask'])\n",
    "\n",
    "    input_ids = np.asarray(input_ids)\n",
    "    attention_masks = np.array(attention_masks)\n",
    "    labels = np.array(labels)\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "def load_and_process(df):\n",
    "    train_data, train_labels, test_data, test_labels = read_dataset(df)\n",
    "\n",
    "    train_input_ids, train_attention_masks, train_labels = data_process(pre_process_dataset(train_data), train_labels)\n",
    "    test_input_ids, test_attention_masks, test_labels = data_process(pre_process_dataset(test_data), test_labels)\n",
    "\n",
    "    return train_input_ids, train_attention_masks, train_labels,\\\n",
    "           test_input_ids, test_attention_masks, test_labels\n",
    "\n",
    "def load_data(df):\n",
    "    # load dataset\n",
    "    train_input_ids, train_attention_masks, train_labels,\\\n",
    "    test_input_ids, test_attention_masks, test_labels = load_and_process(df)\n",
    "\n",
    "    train_df = pd.DataFrame(list(zip(train_input_ids, train_attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "    test_df = pd.DataFrame(list(zip(test_input_ids, test_attention_masks)), columns=['input_ids', 'attention_masks'])\n",
    "    \n",
    "    # import Model and Tokenizer\n",
    "    model = GPT2ForSequenceClassification.from_pretrained('gpt2',num_labels=2)\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    # Tokenization \n",
    "    # for train set\n",
    "    train_seq = torch.tensor(train_df['input_ids'].tolist())\n",
    "    train_mask = torch.tensor(train_df['attention_masks'].tolist())\n",
    "    train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "    # for test set\n",
    "    test_seq = torch.tensor(test_df['input_ids'].tolist())\n",
    "    test_mask = torch.tensor(test_df['attention_masks'].tolist())\n",
    "    test_y = torch.tensor(test_labels.tolist())\n",
    "\n",
    "    # Create DataLoaders \n",
    "    batch_size = 16 #32 \n",
    "    train_data = TensorDataset(train_seq, train_mask, train_y) # wrap tensors\n",
    "    train_sampler = RandomSampler(train_data) # sampler for sampling the data during training\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # dataLoader for train set\n",
    "\n",
    "    return model, batch_size, train_dataloader, train_y, test_y, train_seq, test_seq, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model Function\n",
    "def train(model, batch_size, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    total = len(train_dataloader)\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        step = i+1\n",
    "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n",
    "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\n",
    "        filledLength = int(100 * step // total)\n",
    "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\n",
    "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='') # accuracy={total_accuracy}\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        labels = labels.type(torch.LongTensor) # make sure it labels are int64 type\n",
    "\n",
    "        del batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model.zero_grad() # clear previously calculated gradients\n",
    "        \n",
    "        preds = model(sent_id.to(device), token_type_ids=None, attention_mask=mask.to(device), labels=labels.to(device)) # get model predictions for the current batch\n",
    "\n",
    "        loss = preds[0]\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    avg_loss = total_loss / (len(train_dataloader)*batch_size) # compute the training loss of the epoch\n",
    "\n",
    "    return avg_loss, total_preds # returns the loss and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\wenro\\AppData\\Local\\Temp\\ipykernel_11760\\3869529071.py:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  train_seq = torch.tensor(train_df['input_ids'].tolist())\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./IMDB Dataset.csv\") # read csv\n",
    "\n",
    "# get data\n",
    "gpt2, batch_size, train_dataloader, train_y, test_y, train_seq, test_seq, train_mask, test_mask= load_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wenro\\anaconda3\\envs\\CZ4042_NN_Proj\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 3:\n",
      "Batch 2188/2188 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.02\n",
      "\n",
      "Training Loss: 0.021\n",
      "\n",
      "Epoch 2 / 3:\n",
      "Batch 2188/2188 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01\n",
      "\n",
      "Training Loss: 0.015\n",
      "\n",
      "Epoch 3 / 3:\n",
      "Batch 2188/2188 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01\n",
      "\n",
      "Training Loss: 0.010\n"
     ]
    }
   ],
   "source": [
    "model = gpt2.to(device) # pass the pre-trained GPT2 model to our defined architecture\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr = 5e-5, \n",
    "    eps = 1e-8\n",
    ")\n",
    "\n",
    "# train loop\n",
    "best_loss = float('inf') # set initial loss to infinite\n",
    "epochs = 3\n",
    "epochs = epochs\n",
    "current = 1\n",
    "train_loss_list = []\n",
    "while current <= epochs: # for each epoch\n",
    "\n",
    "    print(f'\\nEpoch {current} / {epochs}:')\n",
    "\n",
    "    train_loss, _ = train(model, batch_size, train_dataloader, optimizer) # train model\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    # save the best model\n",
    "    if os.path.isfile('gpt2_3.pth') == False:\n",
    "        torch.save(model.state_dict(), 'gpt2_3.pth')\n",
    "        best_loss = train_loss_list[current-1]\n",
    "        \n",
    "    if len(train_loss_list) > 1:\n",
    "        if train_loss_list[current-1] < best_loss:\n",
    "            best_loss = train_loss_list[current-1]\n",
    "            torch.save(model.state_dict(), 'gpt2_3.pth')\n",
    "                \n",
    "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n",
    "\n",
    "    current = current + 1\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (Evaluation): 233.0684745311737s\n"
     ]
    }
   ],
   "source": [
    "gpt_model = GPT2ForSequenceClassification.from_pretrained('gpt2',num_labels=2) # init model\n",
    "model = gpt_model.to(device)\n",
    "model.load_state_dict(torch.load(\"./stored_weights/gpt2_epoch_3_batch_16_token_150.pth\")) # load weights\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.eval()\n",
    "\n",
    "batch_size = 16  \n",
    "test_data = TensorDataset(test_seq, test_mask, test_y) # wrap tensors\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size) # dataLoader for train set\n",
    "\n",
    "total_preds = []\n",
    "true_labels = []\n",
    "start = time.time()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    batch = [r.to(device) for r in batch] # push the batch to gpu\n",
    "    sent_id, mask, labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(sent_id, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=mask)\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = labels.to('cpu').numpy()\n",
    "    \n",
    "    total_preds.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "end = time.time()\n",
    "print(f'Time taken (Evaluation): {end-start}s') # eval time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88      8554\n",
      "           1       0.80      0.94      0.86      6446\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.87      0.88      0.87     15000\n",
      "weighted avg       0.88      0.87      0.87     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in total_preds for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "print(classification_report(flat_predictions,flat_true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters (GPT2): 124441344\n"
     ]
    }
   ],
   "source": [
    "gpt_model = GPT2ForSequenceClassification.from_pretrained('gpt2',num_labels=2)\n",
    "model = gpt_model.to(device)\n",
    "model.load_state_dict(torch.load(\"./stored_weights/gpt2_epoch_3_batch_16_token_150.pth\"))\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Trainable Parameters (GPT2): {pytorch_total_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CZ4042_NN_Proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
